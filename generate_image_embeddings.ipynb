{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa2aa6f3",
   "metadata": {},
   "source": [
    "### Setup Environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b7a95f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-12 03:35:07.459910: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-12 03:35:07.497459: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-12 03:35:08.311928: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from src.embeddings import get_embeddings_df\n",
    "from src.satellite_embeddings import generate_satellite_embeddings_df, get_foundational_satellite_embeddings_df\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7173cd2",
   "metadata": {},
   "source": [
    "# Embeddings Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d1d6e4",
   "metadata": {},
   "source": [
    "## Satellite Image Embeddings\n",
    "\n",
    "To extract the embeddings of the satellite images, we can use a set of foundational models available\n",
    "\n",
    "In this case we finetuned a Resnet 50 v2 backbone using a variational autoencoder of the 81 municipalities\n",
    "\n",
    "Here the possible models:  \n",
    "\n",
    "* 'VGG16'\n",
    "* 'MobileNetV2'\n",
    "* 'vit'\n",
    "* 'autoencoder' \n",
    "* 'variational_autoencoder' \n",
    "* 'ResNet50V2' \n",
    "* 'ConvNeXtTiny'\n",
    "\n",
    "For autoencoder and variational autoencoder, you can also use a set of backbones like:\n",
    "* 'vit' \n",
    "* 'ResNet50V2' \n",
    "* 'ConvNeXtTiny'\n",
    "\n",
    "The latent dimension are just for variational autoencoder and autoencoder\n",
    "\n",
    "target_size is the size of the image\n",
    "\n",
    "Important: To use the autoencoder or variational autoencoder you have to first pre-train a model using the file `train_self-supervised.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff55d2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to Dataset\n",
    "path = 'datasets/violence_prediction/Satellite/DATASET/'\n",
    "model_name = 'variational_autoencoder'\n",
    "encoder_backbone = 'ResNet50V2'\n",
    "latent_dim = 1024\n",
    "\n",
    "target_size = (224, 224, 3)\n",
    "\n",
    "# Model path\n",
    "model_path = f'Weights/{model_name}_{encoder_backbone}_{target_size[0]}_{latent_dim}_{target_size[2]}Bands_full_dataset.h5'\n",
    "\n",
    "# Embeddings path\n",
    "if model_name in ['autoencoder', 'variational_autoencoder']:\n",
    "    embeddings_path = f'Embeddings/violence/{model_name}/{model_name}_{encoder_backbone}__{target_size[0]}_{latent_dim}_{target_size[2]}Bands.csv'\n",
    "else:\n",
    "    embeddings_path = f'Embeddings/violence/{model_name}_{target_size[0]}_{latent_dim}.csv'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65022640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images in directories: \n",
      "Cali\n",
      "Cúcuta\n",
      "Villavicencio\n",
      "Barranquilla\n",
      "Ibagué\n",
      "Popayán\n",
      "Soacha\n",
      "Bucaramanga\n",
      "Pasto\n",
      "Medellín\n",
      "WARNING:tensorflow:From /home/datascience/conda/data_fusion_v0_0_1/lib/python3.8/site-packages/keras/src/layers/normalization/batch_normalization.py:883: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-12 00:58:14.544774: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
      "2024-01-12 00:58:14.757481: W tensorflow/c/c_api.cc:304] Operation '{name:'conv5_block1_1_bn/moving_mean/Assign' id:2916 op device:{requested: '', assigned: ''} def:{{{node conv5_block1_1_bn/moving_mean/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](conv5_block1_1_bn/moving_mean, conv5_block1_1_bn/moving_mean/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-12 00:58:19.791282: W tensorflow/c/c_api.cc:304] Operation '{name:'batch_normalization_2/moving_mean/Assign' id:6501 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_2/moving_mean/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_2/moving_mean, batch_normalization_2/moving_mean/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " model (Functional)          (None, 1024)              27761152  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27761152 (105.90 MB)\n",
      "Trainable params: 0 (0.00 Byte)\n",
      "Non-trainable params: 27761152 (105.90 MB)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Municipality Code</th>\n",
       "      <th>Date</th>\n",
       "      <th>Embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cali</td>\n",
       "      <td>2021-09-12</td>\n",
       "      <td>[-1.3068234, -0.8891329, -0.82548416, -1.98768...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cali</td>\n",
       "      <td>2017-07-30</td>\n",
       "      <td>[-0.8662267, -1.9892781, -0.45083773, -1.58955...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cali</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>[-0.16271016, 1.2283182, -0.24431898, 0.053710...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cali</td>\n",
       "      <td>2020-01-26</td>\n",
       "      <td>[1.0123104, -0.17539583, 0.15753587, -1.503514...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cali</td>\n",
       "      <td>2019-10-20</td>\n",
       "      <td>[-1.5866194, 1.0209501, -1.0373107, 0.53583723...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3646</th>\n",
       "      <td>Medellín</td>\n",
       "      <td>2016-09-18</td>\n",
       "      <td>[-0.07596938, -0.20064315, 1.5155005, -0.16436...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3647</th>\n",
       "      <td>Medellín</td>\n",
       "      <td>2020-11-22</td>\n",
       "      <td>[1.9030588, -0.22682817, -0.7058452, 0.6177066...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3648</th>\n",
       "      <td>Medellín</td>\n",
       "      <td>2020-07-26</td>\n",
       "      <td>[1.4073528, 1.4124076, 0.81622416, -0.2352677,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3649</th>\n",
       "      <td>Medellín</td>\n",
       "      <td>2020-09-20</td>\n",
       "      <td>[-0.4603149, -0.1990348, 0.94120204, -1.637651...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3650</th>\n",
       "      <td>Medellín</td>\n",
       "      <td>2018-08-12</td>\n",
       "      <td>[-0.7528285, -1.8154957, 0.5063533, -0.924399,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3651 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Municipality Code        Date  \\\n",
       "0                 Cali  2021-09-12   \n",
       "1                 Cali  2017-07-30   \n",
       "2                 Cali  2020-03-01   \n",
       "3                 Cali  2020-01-26   \n",
       "4                 Cali  2019-10-20   \n",
       "...                ...         ...   \n",
       "3646          Medellín  2016-09-18   \n",
       "3647          Medellín  2020-11-22   \n",
       "3648          Medellín  2020-07-26   \n",
       "3649          Medellín  2020-09-20   \n",
       "3650          Medellín  2018-08-12   \n",
       "\n",
       "                                              Embedding  \n",
       "0     [-1.3068234, -0.8891329, -0.82548416, -1.98768...  \n",
       "1     [-0.8662267, -1.9892781, -0.45083773, -1.58955...  \n",
       "2     [-0.16271016, 1.2283182, -0.24431898, 0.053710...  \n",
       "3     [1.0123104, -0.17539583, 0.15753587, -1.503514...  \n",
       "4     [-1.5866194, 1.0209501, -1.0373107, 0.53583723...  \n",
       "...                                                 ...  \n",
       "3646  [-0.07596938, -0.20064315, 1.5155005, -0.16436...  \n",
       "3647  [1.9030588, -0.22682817, -0.7058452, 0.6177066...  \n",
       "3648  [1.4073528, 1.4124076, 0.81622416, -0.2352677,...  \n",
       "3649  [-0.4603149, -0.1990348, 0.94120204, -1.637651...  \n",
       "3650  [-0.7528285, -1.8154957, 0.5063533, -0.924399,...  \n",
       "\n",
       "[3651 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_satellite_embeddings_df(path, model_name, target_size, latent_dim, encoder_backbone, embeddings_path, model_path=model_path, ignore_black=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef04c45d",
   "metadata": {},
   "source": [
    "### Dino V2 on satellite images\n",
    "As a second example we will extract the embeddings using Dino V2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ac0cc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################  dinov2_large  ##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/datascience/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch number: 10\n",
      "##################################################  dinov2_large  ##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/datascience/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch number: 10\n",
      "##################################################  dinov2_large  ##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/datascience/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch number: 10\n",
      "##################################################  dinov2_large  ##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/datascience/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch number: 10\n",
      "##################################################  dinov2_large  ##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/datascience/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch number: 10\n",
      "##################################################  dinov2_large  ##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/datascience/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch number: 10\n",
      "##################################################  dinov2_large  ##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/datascience/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch number: 10\n",
      "##################################################  dinov2_large  ##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/datascience/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch number: 10\n",
      "##################################################  dinov2_large  ##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/datascience/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch number: 10\n",
      "##################################################  dinov2_large  ##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/datascience/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch number: 10\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "path = 'datasets/violence_prediction/Satellite/DATASET/'\n",
    "backbone = 'dinov2_large'\n",
    "out_dir = 'Embeddings'\n",
    "dataset_name='violence'\n",
    "\n",
    "get_foundational_satellite_embeddings_df(batch_size=batch_size, path=path, dataset_name=dataset_name, backbone=backbone, directory=out_dir, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c304c195",
   "metadata": {},
   "source": [
    "## BRSET Embeddings\n",
    "\n",
    "To extract the embeddings of BRSET we provided a set of foundational models that can be used:\n",
    "\n",
    "\n",
    "* **Batch Size:** Images per batch to convert to embeddings (Adjust depending on your memory)\n",
    "\n",
    "* **Path:** Path to the images\n",
    "\n",
    "* **Output Directory:** Directory to save the embeddings\n",
    "\n",
    "* **Backbone:** Select a backbone from the list of possible backbones:\n",
    "    * 'dinov2_small'\n",
    "    * 'dinov2_base'\n",
    "    * 'dinov2_large'\n",
    "    * 'dinov2_giant'\n",
    "    * 'sam_base'\n",
    "    * 'sam_large'\n",
    "    * 'sam_huge'\n",
    "    * 'clip_base',\n",
    "    * 'clip_large',\n",
    "    * 'convnextv2_tiny'\n",
    "    * 'convnextv2_base'\n",
    "    * 'convnextv2_large'\n",
    "    * 'convnext_tiny'\n",
    "    * 'convnext_small'\n",
    "    * 'convnext_base'\n",
    "    * 'convnext_large'\n",
    "    * 'swin_tiny'\n",
    "    * 'swin_small'\n",
    "    * 'swin_base'\n",
    "    * 'vit_base'\n",
    "    * 'vit_large'\n",
    "    * 'retfound'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcb59167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of possible backbones for BRSET image collection: \n",
      "['dinov2_small', 'dinov2_base', 'dinov2_large', 'dinov2_giant', 'clip_base', 'clip_large', 'sam_base', 'sam_large', 'sam_huge', 'convnextv2_tiny', 'convnextv2_base', 'convnextv2_large', 'convnext_tiny', 'convnext_small', 'convnext_base', 'convnext_large', 'swin_tiny', 'swin_small', 'swin_base', 'vit_base', 'vit_large', 'retfound']\n"
     ]
    }
   ],
   "source": [
    "# Foundational Models\n",
    "dino_backbone = ['dinov2_small', 'dinov2_base', 'dinov2_large', 'dinov2_giant']\n",
    "\n",
    "sam_backbone = ['sam_base', 'sam_large', 'sam_huge']\n",
    "\n",
    "clip_backbone = ['clip_base', 'clip_large']\n",
    "\n",
    "# ImageNet:\n",
    "\n",
    "### Convnext\n",
    "convnext_backbone = ['convnextv2_tiny', 'convnextv2_base', 'convnextv2_large'] + ['convnext_tiny', 'convnext_small', 'convnext_base', 'convnext_large']\n",
    "\n",
    "### Swin Transformer\n",
    "swin_transformer_backbone = ['swin_tiny', 'swin_small', 'swin_base']\n",
    "\n",
    "### ViT\n",
    "vit_backbone = ['vit_base', 'vit_large']\n",
    "\n",
    "### RetFound\n",
    "retfound_backbone = ['retfound']\n",
    "\n",
    "backbones = dino_backbone + clip_backbone + sam_backbone + convnext_backbone + swin_transformer_backbone + vit_backbone + retfound_backbone\n",
    "\n",
    "print(f'List of possible backbones for BRSET image collection: \\n{backbones}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f0483b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################  dinov2_large  ##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/datascience/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch number: 10\n",
      "Processed batch number: 20\n",
      "Processed batch number: 30\n",
      "Processed batch number: 40\n",
      "Processed batch number: 50\n",
      "Processed batch number: 60\n",
      "Processed batch number: 70\n",
      "Processed batch number: 80\n",
      "Processed batch number: 90\n",
      "Processed batch number: 100\n",
      "Processed batch number: 110\n",
      "Processed batch number: 120\n",
      "Processed batch number: 130\n",
      "Processed batch number: 140\n",
      "Processed batch number: 150\n",
      "Processed batch number: 160\n",
      "Processed batch number: 170\n",
      "Processed batch number: 180\n",
      "Processed batch number: 190\n",
      "Processed batch number: 200\n",
      "Processed batch number: 210\n",
      "Processed batch number: 220\n",
      "Processed batch number: 230\n",
      "Processed batch number: 240\n",
      "Processed batch number: 250\n",
      "Processed batch number: 260\n",
      "Processed batch number: 270\n",
      "Processed batch number: 280\n",
      "Processed batch number: 290\n",
      "Processed batch number: 300\n",
      "Processed batch number: 310\n",
      "Processed batch number: 320\n",
      "Processed batch number: 330\n",
      "Processed batch number: 340\n",
      "Processed batch number: 350\n",
      "Processed batch number: 360\n",
      "Processed batch number: 370\n",
      "Processed batch number: 380\n",
      "Processed batch number: 390\n",
      "Processed batch number: 400\n",
      "Processed batch number: 410\n",
      "Processed batch number: 420\n",
      "Processed batch number: 430\n",
      "Processed batch number: 440\n",
      "Processed batch number: 450\n",
      "Processed batch number: 460\n",
      "Processed batch number: 470\n",
      "Processed batch number: 480\n",
      "Processed batch number: 490\n",
      "Processed batch number: 500\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "path = '/home/datascience/Retina/datasets/BRSET/images/'\n",
    "backbone = 'dinov2_large'\n",
    "out_dir = 'Embeddings'\n",
    "dataset_name='BRSET'\n",
    "\n",
    "get_embeddings_df(batch_size=batch_size, path=path, dataset_name=dataset_name, backbone=backbone, directory=out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d090c0-2f34-4247-ad08-4d74d8df9ff1",
   "metadata": {},
   "source": [
    "### Generate MIMIC CXR embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a96f10-5445-4b9f-9d24-7091ccd71e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "path = 'datasets/mimic/images'\n",
    "dataset = 'mimic'\n",
    "backbone = 'dinov2_base'\n",
    "out_dir = 'Embeddings'\n",
    "\n",
    "get_embeddings_df(batch_size=batch_size, path=path, dataset_name=dataset, backbone=backbone, directory=out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c08ff8-ce9f-4fe6-8466-1dc029b8eeea",
   "metadata": {},
   "source": [
    "### Generate HAM 10000 embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecd0baa-a440-456d-b528-18181f485fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "path = 'datasets/ham10000/images'\n",
    "dataset = 'ham10000'\n",
    "backbone = 'dinov2_base'\n",
    "out_dir = 'Embeddings'\n",
    "\n",
    "get_embeddings_df(batch_size=batch_size, path=path, dataset_name=dataset, backbone=backbone, directory=out_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_fusion_v0_0_1]",
   "language": "python",
   "name": "conda-env-data_fusion_v0_0_1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
